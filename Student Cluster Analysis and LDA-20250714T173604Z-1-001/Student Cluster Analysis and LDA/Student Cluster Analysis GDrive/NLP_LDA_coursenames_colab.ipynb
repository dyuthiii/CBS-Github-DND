{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"6JqzfTN6_T6T"},"outputs":[],"source":["# @title\n","!pip install gensim"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"exfGVoIQ_x4R","collapsed":true},"outputs":[],"source":["%pip install pyLDAvis"]},{"cell_type":"code","source":["%pip install \"numpy<2\"\n","%pip install spacy\n","spacy.cli.download(\"en_core_web_sm\")"],"metadata":{"id":"nwQ7jh0689Ty"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"42PaWE7y-PyF"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Tue Jul 29 13:54:48 2025\n","\n","@author: dnd2129\n","NLP processes\n","\"\"\"\n","\n","import pandas as pd\n","\n","import numpy as np\n","\n","import re\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","from nltk.corpus import stopwords\n","from nltk import punkt\n","from nltk import word_tokenize, ngrams\n","from nltk.util import ngrams\n","\n","import spacy\n","\n","import gensim\n","# Sklearn\n","from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.model_selection import GridSearchCV\n","from pprint import pprint\n","# Plotting tools\n","\n","#import pyLDAvis\n","#import pyLDAvis.sklearn\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","source":["from google.colab import auth\n","import gspread\n","from google.auth import default"],"metadata":{"id":"cHNAFVQkBdvS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["auth.authenticate_user()\n","creds, _ = default()\n","gc = gspread.authorize(creds)"],"metadata":{"id":"STg_HGTaBm_e"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":1,"metadata":{"id":"0bTLWd8v-W9e","collapsed":true,"colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"status":"error","timestamp":1756144051273,"user_tz":240,"elapsed":158,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}},"outputId":"070a1f98-417d-4708-f1dc-d2573f06f200"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'pd' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-930613227.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/CBS/CBS-Github-DND/Student Cluster Analysis and LDA-20250714T173604Z-1-001/Student Cluster Analysis and LDA/raw data/Project - Student Course Clustering/Student Course Elective Enrollments Graduates 2016-2025.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#unq_courses= df.combined_name.unique()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}],"source":["df = pd.read_excel(\"/content/drive/MyDrive/CBS/CBS-Github-DND/Student Cluster Analysis and LDA-20250714T173604Z-1-001/Student Cluster Analysis and LDA/raw data/Project - Student Course Clustering/Student Course Elective Enrollments Graduates 2016-2025.xlsx\")\n","df.head()\n","#unq_courses= df.combined_name.unique()\n","\n"]},{"cell_type":"markdown","source":["#NLP Process"],"metadata":{"id":"33qqRFn4aXLz"}},{"cell_type":"markdown","metadata":{"id":"nR_DSlhnxgMg"},"source":["## Cleaning Text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NF-jHUkH-ZdT","executionInfo":{"status":"aborted","timestamp":1756144051256,"user_tz":240,"elapsed":336,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"outputs":[],"source":["\n","#cleaning texts\n","df['clean_name'] = df['combined_name'].fillna('') #changing nan to blank\n","df['clean_name'] = df['clean_name'].str.replace(r'\\s+', ' ', regex=True)#cleaning new line chars\n","df['clean_name'] = df['clean_name'].str.replace(\"'\", '', regex=False)#removing single quotes\n","\n","df.head()\n"]},{"cell_type":"markdown","source":["### Remove stopwords"],"metadata":{"id":"92v6C-uifMNY"}},{"cell_type":"code","source":["def rem_sw(var_in):\n","    sw = list(set(stopwords.words('english')))\n","    tmp = [word for word in var_in.split() if word not in sw]\n","    return ' '.join(tmp)"],"metadata":{"id":"-B7_ujrdfLmT","executionInfo":{"status":"aborted","timestamp":1756144051263,"user_tz":240,"elapsed":341,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['clean_name'] = df['clean_name'].apply(rem_sw)\n","df.head()"],"metadata":{"id":"7eE3gDsAfSA-","executionInfo":{"status":"aborted","timestamp":1756144051266,"user_tz":240,"elapsed":343,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.tail()"],"metadata":{"id":"6vWq31e1mHd0","executionInfo":{"status":"aborted","timestamp":1756144051341,"user_tz":240,"elapsed":417,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## n-gram extraction"],"metadata":{"id":"epO2ShjTeEMZ"}},{"cell_type":"markdown","source":["### Manual Tokenizing::: may be ignore as tfidf may be better!"],"metadata":{"id":"pA805w7TiSyi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bzpsvFEA-cfa","executionInfo":{"status":"aborted","timestamp":1756144051345,"user_tz":240,"elapsed":419,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"outputs":[],"source":["'''\n","Tokenize each sentence into a list of words, removing punctuations and\n","unnecessary characters altogether.\n","\n","Using spaCy because nltk wasn't working-- and according to article, spaCy is better anyway....\n","'''\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","def generate_ngrams_spacy(text):\n","    doc = nlp(text.lower())\n","    tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n","\n","    unigrams = list(zip(tokens))\n","    bigrams = list(zip(tokens, tokens[1:])) if len(tokens) > 1 else []\n","    trigrams = list(zip(tokens, tokens[1:], tokens[2:])) if len(tokens) > 2 else []\n","\n","    return unigrams, bigrams, trigrams\n"]},{"cell_type":"code","source":["df[['unigrams', 'bigrams', 'trigrams']] = df['clean_name'].fillna('').apply(\n","    lambda x: pd.Series(generate_ngrams_spacy(x))\n",")"],"metadata":{"id":"h8DD6NC7iR8V","executionInfo":{"status":"aborted","timestamp":1756144051349,"user_tz":240,"elapsed":422,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"XJI3rFjHktGo","executionInfo":{"status":"aborted","timestamp":1756144051352,"user_tz":240,"elapsed":423,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#df.to_csv('course names uni-bi-trigrams.csv')"],"metadata":{"id":"bVB83RcGqQeJ","executionInfo":{"status":"aborted","timestamp":1756144051355,"user_tz":240,"elapsed":424,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["subset = df[['clean_name', 'unigrams', 'bigrams', 'trigrams']].copy()\n","\n","# Convert unhashable list columns to strings\n","subset['unigrams'] = subset['unigrams'].astype(str)\n","subset['bigrams'] = subset['bigrams'].astype(str)\n","subset['trigrams'] = subset['trigrams'].astype(str)\n","\n","# Now drop duplicates safely\n","unique_subset = subset.drop_duplicates()\n","\n","# Check it\n","unique_subset.head()\n"],"metadata":{"id":"d3wlGHhB0kz-","executionInfo":{"status":"aborted","timestamp":1756144051358,"user_tz":240,"elapsed":424,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#unique_subset.to_csv(\"unique_subset.csv\", index=False)"],"metadata":{"id":"ocG_z_f50-Hf","executionInfo":{"status":"aborted","timestamp":1756144051361,"user_tz":240,"elapsed":425,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Manually looking at unigrams, bigrams and trigrams to decide which to use for LDA"],"metadata":{"id":"jzF64S1c5MXk"}},{"cell_type":"markdown","source":["### Tf-idf"],"metadata":{"id":"X7X7pfnFEehv"}},{"cell_type":"code","source":["# Your existing setup\n","vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n","X = vectorizer.fit_transform(df['clean_name'])\n","\n","# Sum TF-IDF values across all documents\n","tfidf_sums = X.sum(axis=0).A1  # A1 flattens the matrix to a 1D array\n","feature_names = vectorizer.get_feature_names_out()"],"metadata":{"id":"w6luu5CrEgoq","executionInfo":{"status":"aborted","timestamp":1756144051365,"user_tz":240,"elapsed":428,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["gives feature names, i.e., ngrams"],"metadata":{"id":"SWGt8xd2Ez6v"}},{"cell_type":"markdown","source":["getting  SCORE FOR EACH ngram"],"metadata":{"id":"UoDZb09OEuP4"}},{"cell_type":"code","source":["# Create DataFrame of words and scores\n","tfidf_scores = pd.DataFrame({\n","    'term': feature_names,\n","    'score': tfidf_sums\n","})\n","\n","# Sort by score descending\n","tfidf_scores = tfidf_scores.sort_values(by='score', ascending=False)\n","\n","# View top words/phrases\n","print(tfidf_scores.head(30))"],"metadata":{"id":"WhfB2C3pUVy_","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1756144051428,"user_tz":240,"elapsed":10,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}},"outputId":"ff75b456-f9e6-4ba8-e811-5b62ce119f00"},"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'pd' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-652526042.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create DataFrame of words and scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m tfidf_scores = pd.DataFrame({\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m'term'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'score'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtfidf_sums\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m })\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}]},{"cell_type":"markdown","source":["write csv"],"metadata":{"id":"Ec4GrBJ2FMls"}},{"cell_type":"code","source":["tfidf_scores.to_csv(\"tfidf_scores ngrams 1,2,3.csv\", index=False)"],"metadata":{"id":"d6sIpp2qFLwL","executionInfo":{"status":"aborted","timestamp":1756144051375,"user_tz":240,"elapsed":435,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### IDF: maybe this will make it easier to sort out words!\n","higher scores--> less frequent-- may be more valuable to us"],"metadata":{"id":"Mcm_vG1mYgkb"}},{"cell_type":"code","source":["import math\n","import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Step 1: Use CountVectorizer to get n-grams (1 to 3)\n","vectorizer = CountVectorizer(ngram_range=(1, 3), analyzer='word')\n","X = vectorizer.fit_transform(df['clean_name'])\n","\n","# Step 2: Extract n-gram feature names\n","feature_names = vectorizer.get_feature_names_out()\n","\n","# Step 3: Get number of documents each n-gram appears in (document frequency)-- I think this is counting each ngram list as a document, and NOT the clean course name.\n","# This is a binary presence matrix â€” whether term appears in document\n","document_frequencies = (X > 0).sum(axis=0).A1  # .A1 flattens to 1D array\n","num_documents = X.shape[0]\n"],"metadata":{"id":"VQcEpOymYvjh","executionInfo":{"status":"aborted","timestamp":1756144051379,"user_tz":240,"elapsed":438,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(feature_names)"],"metadata":{"id":"5LoqSjlB2RPa","executionInfo":{"status":"aborted","timestamp":1756144051383,"user_tz":240,"elapsed":439,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 4: Compute smoothed IDF scores\n","idf_scores = {\n","    term: math.log((1 + num_documents) / (1 + dfreq)) + 1\n","    for term, dfreq in zip(feature_names, document_frequencies)\n","}"],"metadata":{"id":"k5OfzMJyY4uF","executionInfo":{"status":"aborted","timestamp":1756144051387,"user_tz":240,"elapsed":442,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 5: Convert to DataFrame and sort\n","idf_df = pd.DataFrame(idf_scores.items(), columns=[\"term\", \"idf_score\"])\n","idf_df['total_count_across_ngrams'] = document_frequencies #adding number of documents(coursename) each term appears in\n","#matches substring and \\b ..\\b matches word boundaries... so 'database' won't count in 'data'.\n","#This is re-countign even if it occurs in shorter/bigger ngrams for the same name--- need to get just count.\n","idf_df = idf_df.sort_values(by=\"idf_score\", ascending=False)\n","\n","idf_df.head(20)\n","\n"],"metadata":{"id":"Q5e2LAC1ZHhi","executionInfo":{"status":"aborted","timestamp":1756144051707,"user_tz":240,"elapsed":761,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Add count of unique course_identifiers which have each term"],"metadata":{"id":"x_PfMdxi8rrl"}},{"cell_type":"markdown","source":[" Re-doing clean name with ngrams list linked to clean names using ccount vetorizer instead of manual spacy command like under 'manual tokenization'"],"metadata":{"id":"LjKoW3NXA8tc"}},{"cell_type":"code","source":["def generate_ngrams_countvectorizer(text):\n","  # Combine into a list since CountVectorizer expects an iterable of docs\n","  docs = [text.lower()]\n","\n","  # Unigrams\n","  vectorizer_uni = CountVectorizer(ngram_range=(1, 1))\n","  unigrams = vectorizer_uni.fit(docs)\n","  unigrams_list = list(unigrams.vocabulary_.keys())\n","\n","  # Bigrams\n","  vectorizer_bi = CountVectorizer(ngram_range=(2, 2))\n","  bigrams = vectorizer_bi.fit(docs)\n","  bigrams_list = list(bigrams.vocabulary_.keys())\n","\n","  # Trigrams\n","  vectorizer_tri = CountVectorizer(ngram_range=(3, 3))\n","  trigrams = vectorizer_tri.fit(docs)\n","  trigrams_list = list(trigrams.vocabulary_.keys())\n","\n","  return unigrams_list, bigrams_list, trigrams_list\n","# use\n","df[['unigrams', 'bigrams', 'trigrams']] = df['clean_name'].fillna('').apply(\n","    lambda x: pd.Series(generate_ngrams_spacy(x))\n",")"],"metadata":{"id":"7scNtKTxN77R","executionInfo":{"status":"aborted","timestamp":1756144051712,"user_tz":240,"elapsed":765,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"VyzJ-O_sPK78","executionInfo":{"status":"aborted","timestamp":1756144051718,"user_tz":240,"elapsed":770,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["subset = df[['clean_name', 'unigrams', 'bigrams', 'trigrams']].copy()\n","\n","# Convert unhashable list columns to strings\n","subset['unigrams'] = subset['unigrams'].astype(str)\n","subset['bigrams'] = subset['bigrams'].astype(str)\n","subset['trigrams'] = subset['trigrams'].astype(str)\n","\n","# Now drop duplicates safely\n","unique_subset = subset.drop_duplicates()\n","\n","# Check it\n","unique_subset.head()\n"],"metadata":{"id":"xLv5DFU7A8Qo","executionInfo":{"status":"aborted","timestamp":1756144051725,"user_tz":240,"elapsed":777,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import ast\n","\n","unique_subset['unigrams'] = unique_subset['unigrams'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n","unique_subset['bigrams'] = unique_subset['bigrams'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n","unique_subset['trigrams'] = unique_subset['trigrams'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n"],"metadata":{"id":"mEp4m8S9YPm0","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1756144051835,"user_tz":240,"elapsed":14,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}},"outputId":"30f7e5ba-2b25-4fea-ea2f-681362ee3f34"},"execution_count":3,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'unique_subset' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1842502005.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0munique_subset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unigrams'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_subset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unigrams'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0munique_subset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bigrams'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_subset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bigrams'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0munique_subset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trigrams'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_subset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trigrams'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'unique_subset' is not defined"]}]},{"cell_type":"code","source":["def flatten_and_join(row):\n","    def tuple_to_str(t):\n","        if isinstance(t, (tuple, list)):\n","            return \" \".join(t)\n","        elif isinstance(t, str):\n","            return t\n","        else:\n","            return \"\"\n","\n","    def safe_get(col):\n","        val = row[col]\n","        if isinstance(val, list):\n","            return val\n","        else:\n","            return []\n","\n","    unigrams = safe_get('unigrams')\n","    bigrams = safe_get('bigrams')\n","    trigrams = safe_get('trigrams')\n","\n","    all_ngrams = [tuple_to_str(t) for t in (unigrams + bigrams + trigrams) if t]\n","    return \", \".join(all_ngrams)\n","\n","unique_subset['all_ngrams'] = unique_subset.apply(flatten_and_join, axis=1)\n","\n","unique_subset.head()"],"metadata":{"id":"5FkJdxiHXB7y","executionInfo":{"status":"aborted","timestamp":1756144051771,"user_tz":240,"elapsed":821,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 3: Split combined n-grams into individual terms\n","unique_subset['all_ngrams'] = unique_subset['all_ngrams'].apply(lambda x: [t.strip() for t in x.split(',') if t.strip()])\n","unique_subset.head()"],"metadata":{"id":"uiarK_bMFPty","executionInfo":{"status":"aborted","timestamp":1756144051775,"user_tz":240,"elapsed":825,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Just for quick future use.\n","#unique_subset.to_csv('/content/drive/MyDrive/CBS/CBS-Github-DND/Student Cluster Analysis and LDA-20250714T173604Z-1-001/Student Cluster Analysis and LDA/Student Cluster Analysis GDrive/CV_unique uni-bi-trigram_subset.csv')"],"metadata":{"id":"vn1HNIGfP22S","executionInfo":{"status":"aborted","timestamp":1756144051779,"user_tz":240,"elapsed":828,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Step 4: Flatten into a single list of terms\n","all_terms = [term for terms_list in unique_subset['all_ngrams'] for term in terms_list]\n","\n","# Step 5: Count term occurrences\n","term_counts = pd.Series(all_terms).value_counts().reset_index()\n","term_counts.columns = ['term', 'count']\n","term_counts.head()"],"metadata":{"id":"2LOOsZ4IFUZJ","executionInfo":{"status":"aborted","timestamp":1756144051782,"user_tz":240,"elapsed":830,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["term_counts.tail()"],"metadata":{"id":"kp6pu1jrLkBa","executionInfo":{"status":"aborted","timestamp":1756144051786,"user_tz":240,"elapsed":833,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 6: Merge counts back into your idf_df\n","idf_with_counts = idf_df.merge(term_counts, on='term', how='left')\n","idf_with_counts.head()"],"metadata":{"id":"JY8-R76oFihU","executionInfo":{"status":"aborted","timestamp":1756144051791,"user_tz":240,"elapsed":837,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idf_with_counts.tail(10)"],"metadata":{"id":"PHQ9H1rg8sq8","executionInfo":{"status":"aborted","timestamp":1756144051797,"user_tz":240,"elapsed":842,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 6: Export to CSV\n","idf_with_counts.to_csv(\"/content/drive/MyDrive/CBS/CBS-Github-DND/Student Cluster Analysis and LDA-20250714T173604Z-1-001/Student Cluster Analysis and LDA/Student Cluster Analysis GDrive/idf_ngram_COUNT.csv\", index=False)\n","\n"],"metadata":{"id":"rp-tIo5GZzUr","executionInfo":{"status":"aborted","timestamp":1756144051801,"user_tz":240,"elapsed":842,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### What if i remove verbs?"],"metadata":{"id":"9zyeAtKEwesS"}},{"cell_type":"code","source":["from nltk.tag import pos_tag\n","\n","import nltk\n","nltk.download('averaged_perceptron_tagger_eng')"],"metadata":{"id":"fW9GqSaCxJFk","executionInfo":{"status":"aborted","timestamp":1756144051805,"user_tz":240,"elapsed":845,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tagged_tokens = pos_tag(idf_with_counts['term'])"],"metadata":{"id":"Uz3INmA_xXO6","executionInfo":{"status":"aborted","timestamp":1756144051809,"user_tz":240,"elapsed":848,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idf_with_counts[\"non_verb_tokens\"] = [\n","    word if not tag.startswith('VB') else ''  # or None if you prefer\n","    for word, tag in tagged_tokens\n","]\n","idf_with_counts.head()"],"metadata":{"id":"e8Jc_ZHCwhbR","executionInfo":{"status":"aborted","timestamp":1756144051815,"user_tz":240,"elapsed":854,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### RAKE/YAKE/ Keyword extraction"],"metadata":{"id":"GRTocOhH650Z"}},{"cell_type":"markdown","source":["#### RAKE (Rapid Automatic Keyword extraction)"],"metadata":{"id":"62Jf9K3P7BYs"}},{"cell_type":"code","source":["%pip install rake-nltk\n"],"metadata":{"id":"jfFGh7Hh7BFH","executionInfo":{"status":"aborted","timestamp":1756144051919,"user_tz":240,"elapsed":956,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('punkt_tab')"],"metadata":{"id":"CbyqrWfR7lnW","colab":{"base_uri":"https://localhost:8080/","height":141},"executionInfo":{"status":"error","timestamp":1756144051994,"user_tz":240,"elapsed":13,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}},"outputId":"4abdd34e-80e9-4613-d83e-076395fdb927"},"execution_count":4,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'nltk' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3647733967.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt_tab'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"]}]},{"cell_type":"code","source":["from rake_nltk import Rake\n","\n","rake = Rake()\n","\n","def extract_rake_keywords(text):\n","    rake.extract_keywords_from_text(text)\n","    return rake.get_ranked_phrases()\n","\n","df[\"rake_keywords\"] = df[\"clean_name\"].apply(extract_rake_keywords)\n","df.head()"],"metadata":{"id":"XZaROfkC64XT","executionInfo":{"status":"aborted","timestamp":1756144051944,"user_tz":240,"elapsed":979,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### YAKE (Yet Another Keyword Extractor)"],"metadata":{"id":"mtR5tZP477QE"}},{"cell_type":"code","source":["%pip install yake"],"metadata":{"id":"u0oeg_Wq8K6j","executionInfo":{"status":"aborted","timestamp":1756144051947,"user_tz":240,"elapsed":981,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import yake\n","#YAKE gives all words in ranked order with scores to highlight important keywords-- I just want the top 5 keywords (for simplicity) and i dont care about the score.\n","\n","kw_extractor = yake.KeywordExtractor(top=5)  # limit to top 5 keywords\n","\n","def extract_top5_yake_keywords(text):\n","    keywords = kw_extractor.extract_keywords(text)  # list of (keyword, score) tuples\n","    # Extract only the keywords (ignore scores)\n","    return [kw for kw, score in keywords]\n","\n","df[\"yake_top5_keywords\"] = df[\"clean_name\"].apply(extract_top5_yake_keywords)"],"metadata":{"id":"-ufpTwtT75xS","executionInfo":{"status":"aborted","timestamp":1756144051952,"user_tz":240,"elapsed":985,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df[\"yake_top5_keywords\"].iloc[0]"],"metadata":{"id":"GieO7UINQXcM","executionInfo":{"status":"aborted","timestamp":1756144051961,"user_tz":240,"elapsed":993,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.to_csv(\"/content/drive/MyDrive/CBS/CBS-Github-DND/Student Cluster Analysis and LDA-20250714T173604Z-1-001/Student Cluster Analysis and LDA/Student Cluster Analysis GDrive/ngrams_rake_yake_keywords.csv\")"],"metadata":{"id":"skkymm068P1G","executionInfo":{"status":"aborted","timestamp":1756144051964,"user_tz":240,"elapsed":995,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# LDA processes"],"metadata":{"id":"94KZhrwm6mJL"}},{"cell_type":"markdown","source":["now, combine all courses for each student into one row so we have one row per studnt. Then,tokenize it like I did above (choose uni/bi or trigrams depending of which worked best).\n","THEN: only retain the tokens that I manually chose to use from the manual mapping."],"metadata":{"id":"OCeLnB9j6oOu"}},{"cell_type":"markdown","source":["### Grouping by student\n","One row per student-- adding a course_list column which contains all courses they have taken."],"metadata":{"id":"vym4pyCccWOL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uwCj-_kP-qLR","executionInfo":{"status":"aborted","timestamp":1756144051968,"user_tz":240,"elapsed":998,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"outputs":[],"source":["'''\n","Check the Sparsicity\n","'''\n","# Materialize the sparse data\n","data_dense = data_vectorized.todense()\n","# Compute Sparsicity = Percentage of Non-Zero cells\n","print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")\n","#Sparsicity:  0.6425770811326413 %"]},{"cell_type":"code","source":["#One row per student-- adding a course_list column which contains all courses they have taken.\n","grouped = df.groupby('uni')"],"metadata":{"id":"Zut9_pEkcVzq","executionInfo":{"status":"aborted","timestamp":1756144051976,"user_tz":240,"elapsed":1005,"user":{"displayName":"Dyuthi Dinesh","userId":"05103126141323418953"}}},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["X7X7pfnFEehv","Mcm_vG1mYgkb","9zyeAtKEwesS"],"mount_file_id":"1LPlEk7YN_iTTnMW6GzeevrKcJylsdiH_","authorship_tag":"ABX9TyMBtSZWT0bvRw0DDtAZjP12"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}